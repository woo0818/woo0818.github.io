<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Text2Scene</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://inwoohwang.me/Text2Scene/img/nottingham.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://inwoohwang.me/Text2Scene/" />
    <meta property="og:title" content="Text2Scene: Text-driven Indoor Scene Stylization with Part-aware Details" />
    <meta property="og:description"
        content="Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density. However, these grid-based approaches lack an explicit understanding of scale and therefore often introduce aliasing, usually in the form of jaggies or missing scene content. Anti-aliasing has previously been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone rather than points along a ray, but this approach is not natively compatible with current grid-based techniques. We show how ideas from rendering and signal processing can be used to construct a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP to yield error rates that are 8%-77% lower than either prior technique, and that trains 24x faster than mip-NeRF 360." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Text2Scene: Text-driven Indoor Scene Stylization with Part-aware Details" />
    <meta name="twitter:description"
        content="Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density. However, these grid-based approaches lack an explicit understanding of scale and therefore often introduce aliasing, usually in the form of jaggies or missing scene content. Anti-aliasing has previously been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone rather than points along a ray, but this approach is not natively compatible with current grid-based techniques. We show how ideas from rendering and signal processing can be used to construct a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP to yield error rates that are 8%-77% lower than either prior technique, and that trains 24x faster than mip-NeRF 360." />
    <meta name="twitter:image" content="https://inwoohwang.me/Text2Scene/img/teaser.jpg" />


    <link rel="icon"
        href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>âš¡</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Text2Scene</b>: Text-driven Indoor Scene Stylization with Part-aware Details</br>
                <small>
                    CVPR 2023 (Highlight)
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://inwoohwang.me/">
                            Inwoo Hwang
                        </a>
                    </li>
                    <li>
                        Hyeonwoo Kim
                    </li>
                    <li>
                        <a href="https://3d.snu.ac.kr/members/">
                            Young Min Kim
                        </a>
                    </li>
                    </br>Seoul National University
                </ul>
            </div>
        </div>


        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2308.16880">
                            <image src="img/zip_paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://www.youtube.com/watch?v=CGIXY2kwIYM">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <video id="v0" width="100%" autoplay loop muted controls>
                    <source src="img/teaser.mp4" type="video/mp4" />
                </video>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    We propose Text2Scene, a method to automatically create realistic textures for virtual scenes
                    composed of multiple objects. Guided by a reference image and text descriptions, our pipeline adds
                    detailed texture on labeled 3D geometries in the room such that the generated colors respect the
                    hierarchical structure or semantic parts that are often composed of similar materials. Instead of
                    applying flat stylization on the entire scene at a single step, we obtain weak semantic cues from
                    geometric segmentation, which are further clarified by assigning initial colors to segmented parts.
                    Then we add texture details for individual objects such that their projections on image space
                    exhibit feature embedding aligned with the embedding of the input. The decomposition makes the
                    entire pipeline tractable to a moderate amount of computation resources and memory. As our framework
                    utilizes the existing resources of image and text embedding, it does not require dedicated datasets
                    with high-quality textures designed by skillful artists. To the best of our knowledge, it is the
                    first practical and scalable approach that can create detailed and realistic textures of the desired
                    style that maintain structural context for scenes with multiple objects.
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://youtube.com/embed/CGIXY2kwIYM" allowfullscreen
                            style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Object Stylization
                </h3>
                <br>
                <img src="img/result_object.png" width="100%">

                <br><br>
                <p class="text-justify">

                    Text2Scene generates distinctive textures for different parts, for example, lamp shades, cushions on
                    the sofa, handles of frying pans, of plant pots.
                    The part-aware texture and strong semantic embedding enhance the realism for varioius categories of
                    3D objects.



                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Scene Stylization
                </h3>
                <br>
                <img src="img/result_scene.png" width="100%">

                <br><br>
                <p class="text-justify">

                    Text2Scene is scalable and can create detailed and realistic textures for various indoor scenes.
                    Further, diverse results can be obtained from the same 3D scene and controlled by input conditions.

                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@InProceedings{Hwang_2023_CVPR,
    author    = {Hwang, Inwoo and Kim, Hyeonwoo and Kim, Young Min},
    title     = {Text2Scene: Text-Driven Indoor Scene Stylization With Part-Aware Details},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {1890-1899}
}</textarea>
                </div>
            </div>
        </div>

        <br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">

                <p class="text-justify">


                    The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a> and <a
                        href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>.

                </p>
            </div>
        </div>



    </div>
</body>

</html>